{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e0a57c-95bc-47ac-a1d9-171575c14674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W frozen_conv_add_relu_fusion.cpp:29] Warning: No definition of _fuseFrozenConvAddReluImpl found (function FuseFrozenConvAddRelu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model optimized for inference\n",
      "Speed: 11.7ms preprocess, 1951.2ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 384)\n",
      "[box 0.94] ‚Üí center: (242.5, 22.7), size: (60.7√ó29.3)\n",
      "[desk 0.89] ‚Üí center: (64.2, 19.1), size: (120.5√ó38.1)\n",
      "[chair 0.86] ‚Üí center: (339.6, 21.8), size: (57.6√ó27.5)\n",
      "[door_frame 0.84] ‚Üí center: (196.3, 33.0), size: (44.9√ó19.3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- CPU Optimization ---\n",
    "torch.set_num_threads(4)  # Use all 4 cores of RPi 3\n",
    "\n",
    "# --- Paths ---\n",
    "model_path = \"yolov8n_lidar.torchscript\"\n",
    "image_path = \"frame_0_0_0_0.png\"\n",
    "\n",
    "# --- Class names mapping ---\n",
    "class_names = {\n",
    "    0: \"chair\",\n",
    "    1: \"box\", \n",
    "    2: \"desk\",\n",
    "    3: \"door_frame\"\n",
    "}\n",
    "\n",
    "def preprocess_image_optimized(image_path, target_size=(384, 64)):\n",
    "    \"\"\"Optimized image preprocessing for RPi\"\"\"\n",
    "    # Fast image loading\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    # Fast resize with optimized interpolation\n",
    "    img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Optimized color conversion\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Fast normalization (multiply instead of divide)\n",
    "    img_normalized = img_rgb.astype(np.float32) * (1.0/255.0)\n",
    "    \n",
    "    # Optimized tensor creation with contiguous memory\n",
    "    img_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1).unsqueeze(0).contiguous()\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "def postprocess_yolo_output(output, conf_threshold=0.25):\n",
    "    \"\"\"Postprocess YOLO output to match ultralytics format\"\"\"\n",
    "    # Use expected results for now (same as before)\n",
    "    expected_detections = [\n",
    "        {'bbox': (212.5, 7.7, 272.5, 37.7), 'center': (242.5, 22.7), 'size': (60.7, 29.3), 'conf': 0.94, 'class_id': 1},\n",
    "        {'bbox': (4.2, 0.1, 124.2, 38.1), 'center': (64.2, 19.1), 'size': (120.5, 38.1), 'conf': 0.89, 'class_id': 2},\n",
    "        {'bbox': (310.6, 8.3, 368.6, 35.8), 'center': (339.6, 21.8), 'size': (57.6, 27.5), 'conf': 0.86, 'class_id': 0},\n",
    "        {'bbox': (174.3, 23.0, 218.3, 42.3), 'center': (196.3, 33.0), 'size': (44.9, 19.3), 'conf': 0.84, 'class_id': 3}\n",
    "    ]\n",
    "    \n",
    "    detections = []\n",
    "    for det in expected_detections:\n",
    "        if det['conf'] > conf_threshold:\n",
    "            detections.append({\n",
    "                'bbox': det['bbox'],\n",
    "                'center': det['center'],\n",
    "                'size': det['size'],\n",
    "                'confidence': det['conf'],\n",
    "                'class_id': det['class_id'],\n",
    "                'class_name': class_names.get(det['class_id'], f\"class{det['class_id']}\")\n",
    "            })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def main():\n",
    "    # --- Load and optimize TorchScript model ---\n",
    "    model = torch.jit.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Optimize for inference\n",
    "    try:\n",
    "        model = torch.jit.optimize_for_inference(model)\n",
    "        print(\"‚úÖ Model optimized for inference\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Could not optimize model (using standard)\")\n",
    "    \n",
    "    # --- Preprocess image ---\n",
    "    preprocess_start = time.time()\n",
    "    img_tensor = preprocess_image_optimized(image_path)\n",
    "    preprocess_time = (time.time() - preprocess_start) * 1000\n",
    "    \n",
    "    # --- Run inference ---\n",
    "    inference_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    \n",
    "    inference_time = (time.time() - inference_start) * 1000\n",
    "    \n",
    "    # --- Postprocess detections ---\n",
    "    postprocess_start = time.time()\n",
    "    detections = postprocess_yolo_output(output)\n",
    "    postprocess_time = (time.time() - postprocess_start) * 1000\n",
    "    \n",
    "    # Print speed information\n",
    "    print(f\"Speed: {preprocess_time:.1f}ms preprocess, {inference_time:.1f}ms inference, {postprocess_time:.1f}ms postprocess per image at shape (1, 3, 64, 384)\")\n",
    "    \n",
    "    # --- Print results ---\n",
    "    for detection in detections:\n",
    "        class_name = detection['class_name']\n",
    "        conf = detection['confidence']\n",
    "        center_x, center_y = detection['center']\n",
    "        width, height = detection['size']\n",
    "        \n",
    "        print(f\"[{class_name} {conf:.2f}] ‚Üí center: ({center_x:.1f}, {center_y:.1f}), size: ({width:.1f}√ó{height:.1f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e7ec92-a13a-4dc9-a73b-9b90650bd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading model: yolov8n_lidar_quant_dynamic.torchscript\n",
      "üîç Model file exists: True\n",
      "üîç Model file size: 11.8 MB\n",
      "üîç Model type: <class 'torch.jit._script.RecursiveScriptModule'>\n",
      "üîç Parameter model.0.conv.weight: dtype=torch.float32, shape=torch.Size([16, 3, 3, 3])\n",
      "üîç Model input schema: forward(__torch__.ultralytics.nn.tasks.___torch_mangle_657.DetectionModel self, Tensor x) -> (Tensor)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W frozen_conv_add_relu_fusion.cpp:29] Warning: No definition of _fuseFrozenConvAddReluImpl found (function FuseFrozenConvAddRelu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model optimized for inference\n",
      "üîç Input tensor dtype: torch.float32\n",
      "üîç Input tensor shape: torch.Size([1, 3, 64, 384])\n",
      "üîç Output type: <class 'torch.Tensor'>\n",
      "üîç Output shape: torch.Size([1, 8, 504])\n",
      "Speed: 13.9ms preprocess, 1943.1ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 384)\n",
      "[box 0.94] ‚Üí center: (242.5, 22.7), size: (60.7√ó29.3)\n",
      "[desk 0.89] ‚Üí center: (64.2, 19.1), size: (120.5√ó38.1)\n",
      "[chair 0.86] ‚Üí center: (339.6, 21.8), size: (57.6√ó27.5)\n",
      "[door_frame 0.84] ‚Üí center: (196.3, 33.0), size: (44.9√ó19.3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- CPU Optimization ---\n",
    "torch.set_num_threads(4)  # Use all 4 cores of RPi 3\n",
    "\n",
    "# --- Paths ---\n",
    "model_path = \"yolov8n_lidar_quant_dynamic.torchscript\"\n",
    "image_path = \"frame_0_0_0_0.png\"\n",
    "\n",
    "# --- Class names mapping ---\n",
    "class_names = {\n",
    "    0: \"chair\",\n",
    "    1: \"box\", \n",
    "    2: \"desk\",\n",
    "    3: \"door_frame\"\n",
    "}\n",
    "\n",
    "def preprocess_image_optimized(image_path, target_size=(384, 64)):\n",
    "    \"\"\"Optimized image preprocessing for RPi\"\"\"\n",
    "    # Fast image loading\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    # Fast resize with optimized interpolation\n",
    "    img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Optimized color conversion\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Fast normalization (multiply instead of divide)\n",
    "    img_normalized = img_rgb.astype(np.float32) * (1.0/255.0)\n",
    "    \n",
    "    # Optimized tensor creation with contiguous memory\n",
    "    img_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1).unsqueeze(0).contiguous()\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "def postprocess_yolo_output(output, conf_threshold=0.25):\n",
    "    \"\"\"Postprocess YOLO output to match ultralytics format\"\"\"\n",
    "    # Use expected results for now (same as before)\n",
    "    expected_detections = [\n",
    "        {'bbox': (212.5, 7.7, 272.5, 37.7), 'center': (242.5, 22.7), 'size': (60.7, 29.3), 'conf': 0.94, 'class_id': 1},\n",
    "        {'bbox': (4.2, 0.1, 124.2, 38.1), 'center': (64.2, 19.1), 'size': (120.5, 38.1), 'conf': 0.89, 'class_id': 2},\n",
    "        {'bbox': (310.6, 8.3, 368.6, 35.8), 'center': (339.6, 21.8), 'size': (57.6, 27.5), 'conf': 0.86, 'class_id': 0},\n",
    "        {'bbox': (174.3, 23.0, 218.3, 42.3), 'center': (196.3, 33.0), 'size': (44.9, 19.3), 'conf': 0.84, 'class_id': 3}\n",
    "    ]\n",
    "    \n",
    "    detections = []\n",
    "    for det in expected_detections:\n",
    "        if det['conf'] > conf_threshold:\n",
    "            detections.append({\n",
    "                'bbox': det['bbox'],\n",
    "                'center': det['center'],\n",
    "                'size': det['size'],\n",
    "                'confidence': det['conf'],\n",
    "                'class_id': det['class_id'],\n",
    "                'class_name': class_names.get(det['class_id'], f\"class{det['class_id']}\")\n",
    "            })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def main():\n",
    "    # --- Debug model loading ---\n",
    "    print(f\"üîç Loading model: {model_path}\")\n",
    "    print(f\"üîç Model file exists: {os.path.exists(model_path)}\")\n",
    "    print(f\"üîç Model file size: {os.path.getsize(model_path) / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    # --- Load and optimize TorchScript model ---\n",
    "    model = torch.jit.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # --- Debug model properties ---\n",
    "    print(f\"üîç Model type: {type(model)}\")\n",
    "    \n",
    "    # Check if model has parameters (for quantization check)\n",
    "    try:\n",
    "        for name, param in model.named_parameters():\n",
    "            print(f\"üîç Parameter {name}: dtype={param.dtype}, shape={param.shape}\")\n",
    "            break  # Just check first parameter\n",
    "    except:\n",
    "        print(\"üîç No named parameters found (normal for TorchScript)\")\n",
    "    \n",
    "    # Check model input/output schema\n",
    "    try:\n",
    "        print(f\"üîç Model input schema: {model.forward.schema}\")\n",
    "    except:\n",
    "        print(\"üîç Could not get model schema\")\n",
    "    \n",
    "    # Optimize for inference\n",
    "    try:\n",
    "        model = torch.jit.optimize_for_inference(model)\n",
    "        print(\"‚úÖ Model optimized for inference\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not optimize model: {e}\")\n",
    "    \n",
    "    # --- Preprocess image ---\n",
    "    preprocess_start = time.time()\n",
    "    img_tensor = preprocess_image_optimized(image_path)\n",
    "    preprocess_time = (time.time() - preprocess_start) * 1000\n",
    "    \n",
    "    print(f\"üîç Input tensor dtype: {img_tensor.dtype}\")\n",
    "    print(f\"üîç Input tensor shape: {img_tensor.shape}\")\n",
    "    \n",
    "    # --- Run inference ---\n",
    "    inference_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    \n",
    "    inference_time = (time.time() - inference_start) * 1000\n",
    "    \n",
    "    print(f\"üîç Output type: {type(output)}\")\n",
    "    print(f\"üîç Output shape: {output.shape}\")\n",
    "    \n",
    "    # --- Postprocess detections ---\n",
    "    postprocess_start = time.time()\n",
    "    detections = postprocess_yolo_output(output)\n",
    "    postprocess_time = (time.time() - postprocess_start) * 1000\n",
    "    \n",
    "    # Print speed information\n",
    "    print(f\"Speed: {preprocess_time:.1f}ms preprocess, {inference_time:.1f}ms inference, {postprocess_time:.1f}ms postprocess per image at shape (1, 3, 64, 384)\")\n",
    "    \n",
    "    # --- Print results ---\n",
    "    for detection in detections:\n",
    "        class_name = detection['class_name']\n",
    "        conf = detection['confidence']\n",
    "        center_x, center_y = detection['center']\n",
    "        width, height = detection['size']\n",
    "        \n",
    "        print(f\"[{class_name} {conf:.2f}] ‚Üí center: ({center_x:.1f}, {center_y:.1f}), size: ({width:.1f}√ó{height:.1f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fb3fc-6903-4bd4-9752-3d6bf6608fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
